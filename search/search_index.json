{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"This is the home of the tutorials created by Verifa - your trusted crew for all things Continuous & Cloud.","title":"Introduction"},{"location":"tutorials/","text":"Kubernetes 101 \u00b6 Get a container running in a local Kubernetes cluster in minutes! Start Tutorial Accessing Secret in HashiCorp Vault in Kubernetes \u00b6 Explore various ways of injecting secrets from Vault into your Kubernetes workloads! Start Tutorial","title":"Tutorials"},{"location":"tutorials/#kubernetes-101","text":"Get a container running in a local Kubernetes cluster in minutes! Start Tutorial","title":"Kubernetes 101"},{"location":"tutorials/#accessing-secret-in-hashicorp-vault-in-kubernetes","text":"Explore various ways of injecting secrets from Vault into your Kubernetes workloads! Start Tutorial","title":"Accessing Secret in HashiCorp Vault in Kubernetes"},{"location":"tutorials/kubernetes-101/","tags":["kubernetes","k3d"],"text":"","title":"Kubernetes 101"},{"location":"tutorials/kubernetes-101/cleanup/","text":"Cleanup the cluster \u00b6 To cleanup, we can just tear down the cluster: k3d cluster delete k8s-101","title":"Cleanup"},{"location":"tutorials/kubernetes-101/cleanup/#cleanup-the-cluster","text":"To cleanup, we can just tear down the cluster: k3d cluster delete k8s-101","title":"Cleanup the cluster"},{"location":"tutorials/kubernetes-101/create-deployment/","text":"Deployment is a controller for pods, you create a template for a pod and run X replicas of the pod. In a deployment all the replicas of the pods are stateless, meaning the names of the pod are dynamic. Modify the below manifest to include the pod spec from the pod.yaml : deployment.yaml apiVersion : apps/v1 kind : Deployment metadata : name : veri-deployment labels : app : http-echo spec : replicas : <?> selector : matchLabels : app : http-echo template : metadata : labels : app : http-echo #needs to match with selector for Deployment to find pod! spec : <your-code-comes-here> Again deploy the manifest and use get / describe to view the status of the deployment: kubectl get deployments kubectl get deployment veri-deployment kubectl describe deployment veri-deployment You can refer to official docs for more examples: https://kubernetes.io/docs/concepts/workloads/deployments/ If you're struggling with yaml or short on time, you can get the completed manifest under here deployment.yaml apiVersion : apps/v1 kind : Deployment metadata : name : veri-deployment labels : app : http-echo spec : replicas : 1 selector : matchLabels : app : http-echo template : metadata : labels : app : http-echo #needs to match with selector for Deployment to find pod! spec : containers : - name : http-echo env : - name : ECHO_TEXT value : \"Wohoo\" image : verifa/http-echo:latest ports : - containerPort : 5678","title":"Creating Deployment"},{"location":"tutorials/kubernetes-101/create-k3d-cluster/","text":"# The extra flag maps the localhost:8081 to the loadbalancer container:80 k3d cluster create k8s-101 -p \"8081:80@loadbalancer\" k3d kubeconfig get k8s-101 > k3d-kubeconfig.yaml export KUBECONFIG = $PWD /k3d-kubeconfig.yaml kubectl get pods --namespace kube-system You should wait for all the pods in the kube-system namespace to be Running/Completed, in the rest of the content there's no notion of namespaces anywhere, which means everything will be deployed into the default namespace. This is a very, very, bad practice in production clusters, but it makes our lives bit easier during this tutorial.","title":"Create k3d cluster"},{"location":"tutorials/kubernetes-101/create-pod-cli/","text":"Creating a pod can be done imperatively with a single command: kubectl run --image = alpine alpine Let's see what happened: kubectl get pods kubectl describe pod alpine","title":"Create Pod (CLI)"},{"location":"tutorials/kubernetes-101/create-pod-declarative/","text":"You never should run just Pod in production, when a pod is deleted it's gone forever. Instead of Pod you use a higher level controller such as a Deployment . You can refer to official docs for more examples: https://kubernetes.io/docs/concepts/workloads/pods/ Modify the manifest below to use the verifa/http-echo:latest image: pod.yaml apiVersion : v1 kind : Pod metadata : name : http-echo spec : containers : - name : http-echo image : <HERE> ports : - containerPort : 5678 Note that the containers field is a list, you can specify multiple containers here that share the same IP/network and filesystem. That's why it's a Pod not a container. Apply the manifest using kubectl and use the get and describe verbs to view it: kubectl apply -f pod.yaml kubectl get pods kubectl describe pod http-echo Observe the pod with the commands and see if it starts up, you probably need to spam the commands a few times since it might take a while to pull the image. Expand this after checking the pod status few times Sorry, we set you up for failure. Use a command to check the logs to see what happened: kubectl logs http-echo Hmm, maybe something missing? Add the necessary environment variable to the manifest to get it running: pod.yaml #pod.yaml apiVersion : v1 kind : Pod metadata : name : http-echo spec : containers : - name : http-echo image : <HERE> **env : - name : ECHO_TEXT value : <WRITE_SOMETHING_FUNNY_HERE>** ports : - containerPort : 5678 If you need to you can delete the pod in 2 ways: kubectl delete pod <pod-name> kubectl delete -f pod.yaml For help with manifests You can use the kubectl explain command to help you while writing manifests, try it with: kubectl explain pod.spec.containers kubectl explain pod --recursive Another tool some find helpful is: https://k8syaml.com/ But it does not directly support Pods, since nobody writes pod manifests directly, next we look at what you would write instead.","title":"Creating Pod (Declarative)"},{"location":"tutorials/kubernetes-101/exposing/","text":"Let's explore different ways of exposing the running Pods to the outside world. Service \u00b6 Kubernetes nodes all run a kube-proxy (or something similar) which allows the Service abstraction by proxying the traffic to the correct node where a Pod runs. Although there are actually endpoints between pods and services\u2026 it's complicated, but we don't need to care about Endpoints at all. Remember those labels we set in the pod/deployment spec? They can be used to point a Service to your pod(s): Services come in many flavors, here's a quick overview ClusterIP For intra-cluster traffic, your pods are available behind a single IP and DNS name NodePort For something like an ingress controller, this exposes your service on port X on all nodes LoadBalancer This is typically spawns an external cloud resource (load balancer) pointing to your service, it uses the above two methods inside the cluster and does magic outside cluster to actually route the traffic. You can see the external IP in the cluster in the end, somehow. (well ok, there's a \"Cloud Controller Manager\" in the kube-system namespace, it's typically responsible for this magic show). Let's create a Service of type ClusterIP pointing to our service for fun: service.yaml apiVersion : v1 kind : Service metadata : name : veri-service labels : app : http-echo spec : type : ClusterIP selector : app : http-echo ports : - port : 80 protocol : TCP targetPort : 5678 Now let's try it out using a container that will be removed after we exit ( --rm ): kubectl run -it --rm busybox --image = busybox -- /bin/sh # wait for the prompt, try enter few times wget -O- -q veri-service You might not have noticed, but we actually set the service to expose the pod at port 80 instead of the port the container is configured with. Interesting... Or maybe not \ud83e\udd37 Port-forwarding \u00b6 Exposing the service using ClusterIP didn't really help us to reach it outside the cluster. But we can also just hack the whole thing and use kubectl superpowers to route traffic to any pod: kubectl port-forward deployment/veri-deployment 8080 :5678 Try browsing to http://localhost:8080 and you should see your text there. Ingress \u00b6 Ingress is for fancy L7 routing, similar to an API Gateway that some cloud providers offer. In fact; there's a new Kubernetes API called Gateway API which adds even more features on top of ingress features aiming to be able to be a cloud agnostic yet native API. ingress.yaml apiVersion : networking.k8s.io/v1 kind : Ingress metadata : name : veri-ingress annotations : ingress.kubernetes.io/ssl-redirect : \"false\" spec : rules : - http : paths : - path : /vericonf pathType : Prefix backend : service : name : veri-service port : number : 80 Navigate to http://localhost:8081/ , what do you see? Maybe it is the wrong path actually? Look at the manifest a bit. In addition to routing based on the URL/path, it's possible to configure multiple hosts and route traffic based on subdomains as an example.","title":"Exposing/routing traffic to pods"},{"location":"tutorials/kubernetes-101/exposing/#service","text":"Kubernetes nodes all run a kube-proxy (or something similar) which allows the Service abstraction by proxying the traffic to the correct node where a Pod runs. Although there are actually endpoints between pods and services\u2026 it's complicated, but we don't need to care about Endpoints at all. Remember those labels we set in the pod/deployment spec? They can be used to point a Service to your pod(s): Services come in many flavors, here's a quick overview ClusterIP For intra-cluster traffic, your pods are available behind a single IP and DNS name NodePort For something like an ingress controller, this exposes your service on port X on all nodes LoadBalancer This is typically spawns an external cloud resource (load balancer) pointing to your service, it uses the above two methods inside the cluster and does magic outside cluster to actually route the traffic. You can see the external IP in the cluster in the end, somehow. (well ok, there's a \"Cloud Controller Manager\" in the kube-system namespace, it's typically responsible for this magic show). Let's create a Service of type ClusterIP pointing to our service for fun: service.yaml apiVersion : v1 kind : Service metadata : name : veri-service labels : app : http-echo spec : type : ClusterIP selector : app : http-echo ports : - port : 80 protocol : TCP targetPort : 5678 Now let's try it out using a container that will be removed after we exit ( --rm ): kubectl run -it --rm busybox --image = busybox -- /bin/sh # wait for the prompt, try enter few times wget -O- -q veri-service You might not have noticed, but we actually set the service to expose the pod at port 80 instead of the port the container is configured with. Interesting... Or maybe not \ud83e\udd37","title":"Service"},{"location":"tutorials/kubernetes-101/exposing/#port-forwarding","text":"Exposing the service using ClusterIP didn't really help us to reach it outside the cluster. But we can also just hack the whole thing and use kubectl superpowers to route traffic to any pod: kubectl port-forward deployment/veri-deployment 8080 :5678 Try browsing to http://localhost:8080 and you should see your text there.","title":"Port-forwarding"},{"location":"tutorials/kubernetes-101/exposing/#ingress","text":"Ingress is for fancy L7 routing, similar to an API Gateway that some cloud providers offer. In fact; there's a new Kubernetes API called Gateway API which adds even more features on top of ingress features aiming to be able to be a cloud agnostic yet native API. ingress.yaml apiVersion : networking.k8s.io/v1 kind : Ingress metadata : name : veri-ingress annotations : ingress.kubernetes.io/ssl-redirect : \"false\" spec : rules : - http : paths : - path : /vericonf pathType : Prefix backend : service : name : veri-service port : number : 80 Navigate to http://localhost:8081/ , what do you see? Maybe it is the wrong path actually? Look at the manifest a bit. In addition to routing based on the URL/path, it's possible to configure multiple hosts and route traffic based on subdomains as an example.","title":"Ingress"},{"location":"tutorials/kubernetes-101/overview/","text":"Kubernetes Components \u00b6 Documentation: https://kubernetes.io/docs/concepts/overview/components/ kube-api-server \u00b6 kube-api-server is the front end for the Kubernetes control plane. etcd \u00b6 Consistent and highly-available key value store used as Kubernetes' backing store for all cluster data. kube-scheduler \u00b6 Control plane component that watches for newly created Pods with no assigned node, and selects a node for them to run on. kube-controller-manager \u00b6 Control plane component that runs controller processes. kubelet \u00b6 An agent that runs on each node in the cluster. It makes sure that containers are running in a Pod. kube-proxy \u00b6 kube-proxy is a network proxy that runs on each node in your cluster, implementing part of the Kubernetes Service concept. Reconciliation \u00b6 Example container \u00b6","title":"Kubernetes Overview"},{"location":"tutorials/kubernetes-101/overview/#kubernetes-components","text":"Documentation: https://kubernetes.io/docs/concepts/overview/components/","title":"Kubernetes Components"},{"location":"tutorials/kubernetes-101/overview/#kube-api-server","text":"kube-api-server is the front end for the Kubernetes control plane.","title":"kube-api-server"},{"location":"tutorials/kubernetes-101/overview/#etcd","text":"Consistent and highly-available key value store used as Kubernetes' backing store for all cluster data.","title":"etcd"},{"location":"tutorials/kubernetes-101/overview/#kube-scheduler","text":"Control plane component that watches for newly created Pods with no assigned node, and selects a node for them to run on.","title":"kube-scheduler"},{"location":"tutorials/kubernetes-101/overview/#kube-controller-manager","text":"Control plane component that runs controller processes.","title":"kube-controller-manager"},{"location":"tutorials/kubernetes-101/overview/#kubelet","text":"An agent that runs on each node in the cluster. It makes sure that containers are running in a Pod.","title":"kubelet"},{"location":"tutorials/kubernetes-101/overview/#kube-proxy","text":"kube-proxy is a network proxy that runs on each node in your cluster, implementing part of the Kubernetes Service concept.","title":"kube-proxy"},{"location":"tutorials/kubernetes-101/overview/#reconciliation","text":"","title":"Reconciliation"},{"location":"tutorials/kubernetes-101/overview/#example-container","text":"","title":"Example container"},{"location":"tutorials/kubernetes-101/prerequisites/","text":"Install \u00b6 docker (or similar, such as Rancher desktop ) kubectl k3d Test \u00b6 k3d cluster create k8s-101 k3d kubeconfig get k8s-101 > k3d-kubeconfig.yaml export KUBECONFIG = $PWD /k3d-kubeconfig.yaml kubectl get namespaces k3d cluster delete k8s-101","title":"Prequisites"},{"location":"tutorials/kubernetes-101/prerequisites/#install","text":"docker (or similar, such as Rancher desktop ) kubectl k3d","title":"Install"},{"location":"tutorials/kubernetes-101/prerequisites/#test","text":"k3d cluster create k8s-101 k3d kubeconfig get k8s-101 > k3d-kubeconfig.yaml export KUBECONFIG = $PWD /k3d-kubeconfig.yaml kubectl get namespaces k3d cluster delete k8s-101","title":"Test"},{"location":"tutorials/kubernetes-101/scaling/","text":"For the adventurous you can edit deployments on the fly and modify the replicas field: kubectl edit deployment veri-deployment You can also edit the pod template and that will trigger the pods to be replaced, that might not happen in the nicest way though (unless you've defined a PodDisruptionBudget ). Instead of editing the manifests, Kubernetes ships with some built-in ways to scale and upgrade a Deployment, including things like HorizontalPodAutoscaler. Instead of using automation, we can scale the thing ourselves or do a rolling upgrade: kubectl scale deployment/veri-deployment --replicas = 3 kubectl rollout status deployment/veri-deployment kubectl get pods It might/might not be interesting to you that it's also possible to scale to 0, you can try that as well. We can also change some of the fields such as env or image imperatively: kubectl set env deployment/veri-deployment -e ECHO_TEXT = \"Some other text\" kubectl get deployment/veri-deployment -o yaml | grep -i \"env\\:\" -A2 I think this latest change is actually causing lots of bugs, I'm sure, let's just undo the whole thing and move on: kubectl rollout undo deployment/veri-deployment In production clusters you would rarely use the kubectl set or kubectl rollout commands, but occasionally you might use if upgrading a cluster and bumping some of the image versions of kube-system DaemonSets for example. I'd recommend using GitOps controller such as ArgoCD to manage your application deployments.","title":"Scaling/rolling upgrades"},{"location":"tutorials/vault-k8s/","tags":["kubernetes","HashiCorp","Vault","k3d"],"text":"Hands-on demos exploring various ways of injecting secrets from Vault into your Kubernetes workloads.","title":"Vault secrets in Kubernetes"},{"location":"tutorials/vault-k8s/cleanup/","text":"docker network rm $DOCKER_NETWORK docker rm --force dev-vault k3d cluster delete vault-k8s rm -rf /tmp/k3d rm k3d-kubeconfig.yaml unset KUBECONFIG","title":"Cleanup"},{"location":"tutorials/vault-k8s/create-external-vault/","text":"A lot of tutorials install Vault inside the Kubernetes cluster using the Helm chart, but doing so skips some of the setup necessary when integrating with an external Vault. To make the tutorial more interesting let's run an external Vault inside a Docker container, it's important to note that the Vault and k3d cluster is running inside the same Docker network, thus the Vault can be reached using the name of the docker container. export VAULT_TOKEN = \"root\" # in dev mode we can set the value for root token VAULT_VERSION = \"1.12.0\" docker run --cap-add = IPC_LOCK -p 8200 :8200 -d --name = dev-vault -e \"VAULT_DEV_ROOT_TOKEN_ID= ${ VAULT_TOKEN } \" --network ${ DOCKER_NETWORK } vault: ${ VAULT_VERSION } export VAULT_ADDR = http:// $( docker inspect dev-vault | jq -r \".[0].NetworkSettings.Networks.\\\" ${ DOCKER_NETWORK } \\\".IPAddress\" ) :8200 Test the Vault locally \u00b6 vault status vault token lookup Add some secrets \u00b6 vault kv put secret/foo key = s3cr3t vault kv put secret/bar password = verisecret","title":"Create an external Vault using Docker"},{"location":"tutorials/vault-k8s/create-external-vault/#test-the-vault-locally","text":"vault status vault token lookup","title":"Test the Vault locally"},{"location":"tutorials/vault-k8s/create-external-vault/#add-some-secrets","text":"vault kv put secret/foo key = s3cr3t vault kv put secret/bar password = verisecret","title":"Add some secrets"},{"location":"tutorials/vault-k8s/create-k3d-cluster/","text":"Firstly set/create some common config: export DOCKER_NETWORK = \"k3d-vault-net\" # create a dedicated docker network docker network create $DOCKER_NETWORK mkdir -p /tmp/k3d/kubelet/pods echo \"--- apiVersion: k3d.io/v1alpha4 kind: Simple metadata: name: vault-k8s servers: 1 agents: 2 network: $DOCKER_NETWORK volumes: # needed for the CSI driver - volume: /tmp/k3d/kubelet/pods:/var/lib/kubelet/pods:shared\" > cluster.yaml Create the cluster: k3d cluster create --config cluster.yaml \\ --api-port $( ip route get 8 .8.8.8 | awk '{print $7}' ) :16550 Explicitly fetch the kubeconfig to a file to make sure you connect to the right cluster with kubectl : k3d kubeconfig get vault-k8s > k3d-kubeconfig.yaml export KUBECONFIG = $PWD /k3d-kubeconfig.yaml # test the kubeconfig works and cluster is running kubectl get pods --namespace kube-system You should wait for all the pods in the kube-system namespace to be Running/Completed.","title":"Create k3d cluster"},{"location":"tutorials/vault-k8s/csi-driver/","text":"helm repo add secrets-store-csi-driver https://kubernetes-sigs.github.io/secrets-store-csi-driver/charts helm repo update helm install csi secrets-store-csi-driver/secrets-store-csi-driver --debug --version 1 .2.4 echo \"Waiting for the pods to be Ready..\" kubectl wait --for = condition = Ready pod -l app = secrets-store-csi-driver --timeout = 60s kubectl get pods -l app = secrets-store-csi-driver Vault Terraform configuration \u00b6 Create a new folder to hold the Terraform configuration for this section: cd .. mkdir csi-demo cd csi-demo Create main.tf which holds all the Terraform configuration: main.tf provider \"vault\" { # Configured with environment variables: # VAULT_ADDR # VAULT_TOKEN } resource \"vault_policy\" \"csi-app\" { name = \"csi-app\" policy = <<EOT path \"secret/data/bar\" { capabilities = [\"read\"] } EOT } resource \"vault_kubernetes_auth_backend_role\" \"csi-app\" { backend = \"kubernetes\" # default path role_name = \"csi-app\" bound_service_account_names = [\"csi-app\"] bound_service_account_namespaces = [\"default\"] token_ttl = 3600 token_policies = [\"csi-app\"] audience = \"k3s\" } Apply the terraform configuration after reviewing the file and the plan: terraform init terraform apply Configuring SecretProviderClass \u00b6 spc-vault.yaml apiVersion : secrets-store.csi.x-k8s.io/v1 kind : SecretProviderClass metadata : name : vault-csi-app spec : provider : vault parameters : vaultAddress : \"http://dev-vault:8200\" roleName : \"csi-app\" objects : | - objectName: \"bar-password\" secretPath: \"secret/data/bar\" secretKey: \"password\" CSI Demo Application \u00b6 csi-demo-app.yaml --- apiVersion : v1 kind : ServiceAccount metadata : name : csi-app --- kind : Pod apiVersion : v1 metadata : name : csi-app labels : app : csi-app spec : serviceAccountName : csi-app containers : - image : nginx name : csi-app volumeMounts : - name : secrets-store-inline mountPath : \"/mnt/secrets-store\" readOnly : true volumes : - name : secrets-store-inline csi : driver : secrets-store.csi.k8s.io readOnly : true volumeAttributes : secretProviderClass : \"vault-csi-app\" After the container is running we can examine the secret written: kubectl exec csi-app -- cat /mnt/secrets-store/bar-password && echo","title":"Vault CSI driver"},{"location":"tutorials/vault-k8s/csi-driver/#vault-terraform-configuration","text":"Create a new folder to hold the Terraform configuration for this section: cd .. mkdir csi-demo cd csi-demo Create main.tf which holds all the Terraform configuration: main.tf provider \"vault\" { # Configured with environment variables: # VAULT_ADDR # VAULT_TOKEN } resource \"vault_policy\" \"csi-app\" { name = \"csi-app\" policy = <<EOT path \"secret/data/bar\" { capabilities = [\"read\"] } EOT } resource \"vault_kubernetes_auth_backend_role\" \"csi-app\" { backend = \"kubernetes\" # default path role_name = \"csi-app\" bound_service_account_names = [\"csi-app\"] bound_service_account_namespaces = [\"default\"] token_ttl = 3600 token_policies = [\"csi-app\"] audience = \"k3s\" } Apply the terraform configuration after reviewing the file and the plan: terraform init terraform apply","title":"Vault Terraform configuration"},{"location":"tutorials/vault-k8s/csi-driver/#configuring-secretproviderclass","text":"spc-vault.yaml apiVersion : secrets-store.csi.x-k8s.io/v1 kind : SecretProviderClass metadata : name : vault-csi-app spec : provider : vault parameters : vaultAddress : \"http://dev-vault:8200\" roleName : \"csi-app\" objects : | - objectName: \"bar-password\" secretPath: \"secret/data/bar\" secretKey: \"password\"","title":"Configuring SecretProviderClass"},{"location":"tutorials/vault-k8s/csi-driver/#csi-demo-application","text":"csi-demo-app.yaml --- apiVersion : v1 kind : ServiceAccount metadata : name : csi-app --- kind : Pod apiVersion : v1 metadata : name : csi-app labels : app : csi-app spec : serviceAccountName : csi-app containers : - image : nginx name : csi-app volumeMounts : - name : secrets-store-inline mountPath : \"/mnt/secrets-store\" readOnly : true volumes : - name : secrets-store-inline csi : driver : secrets-store.csi.k8s.io readOnly : true volumeAttributes : secretProviderClass : \"vault-csi-app\" After the container is running we can examine the secret written: kubectl exec csi-app -- cat /mnt/secrets-store/bar-password && echo","title":"CSI Demo Application"},{"location":"tutorials/vault-k8s/external-secrets-operator/","text":"TODO \u00b6 eso-install.sh helm repo add external-secrets https://charts.external-secrets.io helm install external-secrets \\ external-secrets/external-secrets \\ -n external-secrets \\ --create-namespace \\ --debug \\ --set installCRDs = true echo \"Waiting for the pods to be Ready..\" kubectl wait --for = condition = Ready pod -l \"app.kubernetes.io/instance=external-secrets\" -n external-secrets --timeout = 60s kubectl get pods -l \"app.kubernetes.io/instance=external-secrets\" -n external-secrets Vault Terraform configuration \u00b6 vault-eso.tf provider \"vault\" { # Configured with environment variables: # VAULT_ADDR # VAULT_TOKEN } resource \"vault_policy\" \"ext-secrets\" { name = \"ext-secrets\" policy = <<EOT path \"secret/data/foo\" { capabilities = [\"read\"] } EOT } resource \"vault_kubernetes_auth_backend_role\" \"database\" { backend = \"kubernetes\" # default path role_name = \"ext-secrets\" bound_service_account_names = [\"ext-secrets-sa\"] bound_service_account_namespaces = [\"external-secrets\"] token_ttl = 3600 token_policies = [\"ext-secrets\"] # external-secrets operator does something strange with audience, so let it be null: #audience = \"k3s\" } ext-secret-vault.yaml --- apiVersion : external-secrets.io/v1beta1 kind : ExternalSecret metadata : name : vault-example spec : refreshInterval : \"15s\" secretStoreRef : name : vault-backend kind : SecretStore target : name : example-sync data : - secretKey : foobar remoteRef : key : secret/foo property : my-value secret-store-vault.yaml --- apiVersion : v1 kind : ServiceAccount metadata : name : ext-secrets-sa --- apiVersion : external-secrets.io/v1beta1 kind : SecretStore metadata : name : vault-backend spec : provider : vault : server : \"http://dev-vault:8200\" path : \"secret\" version : \"v2\" auth : kubernetes : mountPath : \"kubernetes\" role : \"ext-secrets\" serviceAccountRef : name : \"ext-secrets-sa\"","title":"External Secrets Operator"},{"location":"tutorials/vault-k8s/external-secrets-operator/#todo","text":"eso-install.sh helm repo add external-secrets https://charts.external-secrets.io helm install external-secrets \\ external-secrets/external-secrets \\ -n external-secrets \\ --create-namespace \\ --debug \\ --set installCRDs = true echo \"Waiting for the pods to be Ready..\" kubectl wait --for = condition = Ready pod -l \"app.kubernetes.io/instance=external-secrets\" -n external-secrets --timeout = 60s kubectl get pods -l \"app.kubernetes.io/instance=external-secrets\" -n external-secrets","title":"TODO"},{"location":"tutorials/vault-k8s/external-secrets-operator/#vault-terraform-configuration","text":"vault-eso.tf provider \"vault\" { # Configured with environment variables: # VAULT_ADDR # VAULT_TOKEN } resource \"vault_policy\" \"ext-secrets\" { name = \"ext-secrets\" policy = <<EOT path \"secret/data/foo\" { capabilities = [\"read\"] } EOT } resource \"vault_kubernetes_auth_backend_role\" \"database\" { backend = \"kubernetes\" # default path role_name = \"ext-secrets\" bound_service_account_names = [\"ext-secrets-sa\"] bound_service_account_namespaces = [\"external-secrets\"] token_ttl = 3600 token_policies = [\"ext-secrets\"] # external-secrets operator does something strange with audience, so let it be null: #audience = \"k3s\" } ext-secret-vault.yaml --- apiVersion : external-secrets.io/v1beta1 kind : ExternalSecret metadata : name : vault-example spec : refreshInterval : \"15s\" secretStoreRef : name : vault-backend kind : SecretStore target : name : example-sync data : - secretKey : foobar remoteRef : key : secret/foo property : my-value secret-store-vault.yaml --- apiVersion : v1 kind : ServiceAccount metadata : name : ext-secrets-sa --- apiVersion : external-secrets.io/v1beta1 kind : SecretStore metadata : name : vault-backend spec : provider : vault : server : \"http://dev-vault:8200\" path : \"secret\" version : \"v2\" auth : kubernetes : mountPath : \"kubernetes\" role : \"ext-secrets\" serviceAccountRef : name : \"ext-secrets-sa\"","title":"Vault Terraform configuration"},{"location":"tutorials/vault-k8s/kubernetes-auth/","text":"Kubernetes service account token are used by Kubernetes workloads to authenticate with Vault, in order for Vault to verify the service account tokens Vault must also be able to authenticate with the Kubernetes API server. tip: How to create the service account manually (not needed for the tutorial) The service account needs to have the clusterrole system:auth-delegator . Here are example commands to create it: kubectl create serviceaccount vault kubectl create clusterrolebinding vault-reviewer-binding --clusterrole = system:auth-delegator --serviceaccount = default:vault Make sure to use a long-lived token of this service account instead of a short lived one, example is shown below. Refer to Kubernetes documentation for the details As part of the sidecar injector installation, the Helm chart also created a service account with the necessary role. We can use the service account instead of creating a new one: # create long lived token for the service account kubectl apply -f - <<EOF apiVersion: v1 kind: Secret metadata: name: vault-token annotations: kubernetes.io/service-account.name: vault type: kubernetes.io/service-account-token EOF export TF_VAR_token_reviewer_jwt = $( kubectl get secret vault-token --output = 'go-template={{ .data.token }}' | base64 --decode ) export TF_VAR_kubernetes_ca_cert = $( kubectl config view --raw --minify --flatten --output = 'jsonpath={.clusters[].cluster.certificate-authority-data}' ) export TF_VAR_kubernetes_host = $( kubectl config view --raw --minify --flatten --output = 'jsonpath={.clusters[].cluster.server}' ) Vault Terraform configuration \u00b6 Create a new folder to hold the Terraform configuration for this section: mkdir k8s-auth cd k8s-auth Create main.tf which holds all the Terraform configuration: main.tf provider \"vault\" { # Configured with environment variables: # VAULT_ADDR # VAULT_TOKEN } variable \"kubernetes_host\" { type = string description = \"URL for the Kubernetes API.\" } variable \"kubernetes_ca_cert\" { type = string description = \"Base64 encoded CA certificate of the cluster.\" } variable \"token_reviewer_jwt\" { type = string description = \"JWT token of the Vault Service Account.\" } resource \"vault_auth_backend\" \"this\" { type = \"kubernetes\" } resource \"vault_kubernetes_auth_backend_config\" \"this\" { backend = vault_auth_backend.this.path kubernetes_host = var.kubernetes_host kubernetes_ca_cert = base64decode(var.kubernetes_ca_cert) token_reviewer_jwt = var.token_reviewer_jwt issuer = \"api\" disable_iss_validation = \"true\" # k8s API checks it } resource \"vault_kubernetes_auth_backend_role\" \"default\" { backend = vault_auth_backend.this.path role_name = \"default\" bound_service_account_names = [\"default\"] bound_service_account_namespaces = [\"default\"] token_ttl = 3600 token_policies = [\"default\"] audience = \"k3s\" } Apply the terraform configuration after reviewing the file and the plan: terraform init terraform apply Test the Kubernetes auth \u00b6 In order to test the authentication method we can create a pod that uses the default service account (role for it configured above as part of the auth method): login-app.yaml apiVersion : v1 kind : Pod metadata : name : login-app labels : app : login-app spec : # when not provided 'default' service account will be used #serviceAccountName: login-app containers : - name : app image : hashicorp/vault:1.10.0 env : - name : VAULT_ADDR value : \"http://dev-vault:8200\" command : [ \"/bin/sh\" , \"-c\" ] args : - | TOKEN=$(cat /var/run/secrets/kubernetes.io/serviceaccount/token) echo \"vault write auth/kubernetes/login role=default jwt=$TOKEN\" > /tmp/script/run.sh sh /tmp/script/run.sh && sleep 600 volumeMounts : - mountPath : /tmp/script name : script volumes : - name : script emptyDir : medium : Memory When the pod launches it will use the mounted service account token to authenticate against Vault. You can verify the authentication is working by examining the logs after the pod has authenticated: kubectl logs login-app Here's an example output from a successful authentication: Key Value --- ----- token hvs.CAESIBo4XzzMrUP3VfRfug5m9SrqdAtHXh9azKBt94Aw-Tp1Gh4KHGh2cy55czU1VUZwVUp5R25Sd2RnbWZhZmxmRmI token_accessor wQoJ359SrX8RwRxg0VqyjATS token_duration 1h token_renewable true token_policies [ \"default\" ] identity_policies [] policies [ \"default\" ] token_meta_service_account_name default token_meta_service_account_namespace default token_meta_service_account_secret_name n/a token_meta_service_account_uid c4636638-7d29-471c-a88d-33f167937b2d token_meta_role default The Vault token can be used to fetch secrets from Vault, but this token is only associated with the default policy in Vault which does not grant access to any secrets. In the next sections we will take a look at accessing actual secrets.","title":"Kubernetes authentication method"},{"location":"tutorials/vault-k8s/kubernetes-auth/#vault-terraform-configuration","text":"Create a new folder to hold the Terraform configuration for this section: mkdir k8s-auth cd k8s-auth Create main.tf which holds all the Terraform configuration: main.tf provider \"vault\" { # Configured with environment variables: # VAULT_ADDR # VAULT_TOKEN } variable \"kubernetes_host\" { type = string description = \"URL for the Kubernetes API.\" } variable \"kubernetes_ca_cert\" { type = string description = \"Base64 encoded CA certificate of the cluster.\" } variable \"token_reviewer_jwt\" { type = string description = \"JWT token of the Vault Service Account.\" } resource \"vault_auth_backend\" \"this\" { type = \"kubernetes\" } resource \"vault_kubernetes_auth_backend_config\" \"this\" { backend = vault_auth_backend.this.path kubernetes_host = var.kubernetes_host kubernetes_ca_cert = base64decode(var.kubernetes_ca_cert) token_reviewer_jwt = var.token_reviewer_jwt issuer = \"api\" disable_iss_validation = \"true\" # k8s API checks it } resource \"vault_kubernetes_auth_backend_role\" \"default\" { backend = vault_auth_backend.this.path role_name = \"default\" bound_service_account_names = [\"default\"] bound_service_account_namespaces = [\"default\"] token_ttl = 3600 token_policies = [\"default\"] audience = \"k3s\" } Apply the terraform configuration after reviewing the file and the plan: terraform init terraform apply","title":"Vault Terraform configuration"},{"location":"tutorials/vault-k8s/kubernetes-auth/#test-the-kubernetes-auth","text":"In order to test the authentication method we can create a pod that uses the default service account (role for it configured above as part of the auth method): login-app.yaml apiVersion : v1 kind : Pod metadata : name : login-app labels : app : login-app spec : # when not provided 'default' service account will be used #serviceAccountName: login-app containers : - name : app image : hashicorp/vault:1.10.0 env : - name : VAULT_ADDR value : \"http://dev-vault:8200\" command : [ \"/bin/sh\" , \"-c\" ] args : - | TOKEN=$(cat /var/run/secrets/kubernetes.io/serviceaccount/token) echo \"vault write auth/kubernetes/login role=default jwt=$TOKEN\" > /tmp/script/run.sh sh /tmp/script/run.sh && sleep 600 volumeMounts : - mountPath : /tmp/script name : script volumes : - name : script emptyDir : medium : Memory When the pod launches it will use the mounted service account token to authenticate against Vault. You can verify the authentication is working by examining the logs after the pod has authenticated: kubectl logs login-app Here's an example output from a successful authentication: Key Value --- ----- token hvs.CAESIBo4XzzMrUP3VfRfug5m9SrqdAtHXh9azKBt94Aw-Tp1Gh4KHGh2cy55czU1VUZwVUp5R25Sd2RnbWZhZmxmRmI token_accessor wQoJ359SrX8RwRxg0VqyjATS token_duration 1h token_renewable true token_policies [ \"default\" ] identity_policies [] policies [ \"default\" ] token_meta_service_account_name default token_meta_service_account_namespace default token_meta_service_account_secret_name n/a token_meta_service_account_uid c4636638-7d29-471c-a88d-33f167937b2d token_meta_role default The Vault token can be used to fetch secrets from Vault, but this token is only associated with the default policy in Vault which does not grant access to any secrets. In the next sections we will take a look at accessing actual secrets.","title":"Test the Kubernetes auth"},{"location":"tutorials/vault-k8s/prerequisites/","text":"Warning Unfortunately this tutorial is tested only on Linux since the CSI driver only works on Linux. The tutorial should work on most distributions, as long as k3d supports it. Install \u00b6 vault (for CLI) docker (or similar, such as Rancher desktop ) kubectl k3d jq terraform kubectl helm >=3.6 Test k3d \u00b6 k3d cluster create vault-k8s k3d kubeconfig get vault-k8s > k3d-kubeconfig.yaml export KUBECONFIG = $PWD /k3d-kubeconfig.yaml kubectl get namespaces # cleanup the test cluster and configs k3d cluster delete vault-k8s rm k3d-kubeconfig.yaml unset KUBECONFIG","title":"Prequisites"},{"location":"tutorials/vault-k8s/prerequisites/#install","text":"vault (for CLI) docker (or similar, such as Rancher desktop ) kubectl k3d jq terraform kubectl helm >=3.6","title":"Install"},{"location":"tutorials/vault-k8s/prerequisites/#test-k3d","text":"k3d cluster create vault-k8s k3d kubeconfig get vault-k8s > k3d-kubeconfig.yaml export KUBECONFIG = $PWD /k3d-kubeconfig.yaml kubectl get namespaces # cleanup the test cluster and configs k3d cluster delete vault-k8s rm k3d-kubeconfig.yaml unset KUBECONFIG","title":"Test k3d"},{"location":"tutorials/vault-k8s/sidecar-injector-demo/","text":"Vault Terraform configuration \u00b6 Create a new folder to hold the Terraform configuration for this section: cd .. mkdir injector-demo cd injector-demo Create main.tf which holds all the Terraform configuration: main.tf provider \"vault\" { # Configured with environment variables: # VAULT_ADDR # VAULT_TOKEN } resource \"vault_policy\" \"devweb-app\" { name = \"devwebapp\" policy = <<EOT path \"secret/data/devwebapp/config\" { capabilities = [\"read\"] } EOT } resource \"vault_kubernetes_auth_backend_role\" \"devweb-app\" { backend = \"kubernetes\" # default path role_name = \"devweb-app\" bound_service_account_names = [\"internal-app\"] bound_service_account_namespaces = [\"default\"] token_ttl = 3600 token_policies = [\"devwebapp\"] audience = \"k3s\" } Apply the terraform configuration after reviewing the file and the plan: terraform init terraform apply Sidecar injector \u00b6 Let's define a Pod manifest that accesses the secret using annotations for the configuration: sidecar-demo.yaml apiVersion : v1 kind : ServiceAccount metadata : name : sidecar-app --- apiVersion : v1 kind : Pod metadata : name : sidecar-app labels : app : sidecar-app annotations : vault.hashicorp.com/agent-inject : 'true' vault.hashicorp.com/role : 'sidecar' vault.hashicorp.com/agent-inject-secret-credentials.txt : 'secret/data/foo' vault.hashicorp.com/agent-inject-template-credentials.txt : | {{- with secret \"secret/data/foo\" -}} secret-value: {{ .Data.data.key }} {{- end }} spec : serviceAccountName : sidecar-app containers : - name : sidecar-app image : nginx After the container is running we can examine the secret written: kubectl exec sidecar-app -c sidecar-app -- cat /vault/secrets/credentials.txt","title":"Vault Agent sidecar injector demo"},{"location":"tutorials/vault-k8s/sidecar-injector-demo/#vault-terraform-configuration","text":"Create a new folder to hold the Terraform configuration for this section: cd .. mkdir injector-demo cd injector-demo Create main.tf which holds all the Terraform configuration: main.tf provider \"vault\" { # Configured with environment variables: # VAULT_ADDR # VAULT_TOKEN } resource \"vault_policy\" \"devweb-app\" { name = \"devwebapp\" policy = <<EOT path \"secret/data/devwebapp/config\" { capabilities = [\"read\"] } EOT } resource \"vault_kubernetes_auth_backend_role\" \"devweb-app\" { backend = \"kubernetes\" # default path role_name = \"devweb-app\" bound_service_account_names = [\"internal-app\"] bound_service_account_namespaces = [\"default\"] token_ttl = 3600 token_policies = [\"devwebapp\"] audience = \"k3s\" } Apply the terraform configuration after reviewing the file and the plan: terraform init terraform apply","title":"Vault Terraform configuration"},{"location":"tutorials/vault-k8s/sidecar-injector-demo/#sidecar-injector","text":"Let's define a Pod manifest that accesses the secret using annotations for the configuration: sidecar-demo.yaml apiVersion : v1 kind : ServiceAccount metadata : name : sidecar-app --- apiVersion : v1 kind : Pod metadata : name : sidecar-app labels : app : sidecar-app annotations : vault.hashicorp.com/agent-inject : 'true' vault.hashicorp.com/role : 'sidecar' vault.hashicorp.com/agent-inject-secret-credentials.txt : 'secret/data/foo' vault.hashicorp.com/agent-inject-template-credentials.txt : | {{- with secret \"secret/data/foo\" -}} secret-value: {{ .Data.data.key }} {{- end }} spec : serviceAccountName : sidecar-app containers : - name : sidecar-app image : nginx After the container is running we can examine the secret written: kubectl exec sidecar-app -c sidecar-app -- cat /vault/secrets/credentials.txt","title":"Sidecar injector"},{"location":"tutorials/vault-k8s/sidecar-injector-install/","text":"Vault sidecar injector can be installed with the official Vault Helm chart. It adds a mutating webhook controller into the cluster that modifies pod definitions adding the sidecar container to your Kubernetes manifests. Configuring service entry for Vault \u00b6 To make sure our pods can resolve the name dev-vault to the Vault address let's add a Service and manual Endpoints resources to the cluster: kubectl apply -f - <<EOF apiVersion: v1 kind: Service metadata: name: dev-vault spec: ports: - name: http protocol: TCP port: 8200 targetPort: 8200 --- apiVersion: v1 kind: Endpoints metadata: name: dev-vault subsets: - addresses: - ip: $(docker inspect dev-vault | jq -r \".[0].NetworkSettings.Networks.\\\"${DOCKER_NETWORK}\\\".IPAddress\") ports: - name: http port: 8200 protocol: TCP EOF Note Depending on your version of the tools and overall setup, this might not be needed but we've included it to make the tutorial stable. Installing the Helm chart \u00b6 Install Vault Helm chart which connects to the external Vault. Note that dev-vault is the name we used for the service, and is also the name of the docker container. helm repo add hashicorp https://helm.releases.hashicorp.com helm repo update helm install vault hashicorp/vault \\ --set \"global.externalVaultAddr=http://dev-vault:8200\" \\ --set \"csi.enabled=true\" \\ --version 0 .22.1","title":"Vault Agent sidecar injector installation"},{"location":"tutorials/vault-k8s/sidecar-injector-install/#configuring-service-entry-for-vault","text":"To make sure our pods can resolve the name dev-vault to the Vault address let's add a Service and manual Endpoints resources to the cluster: kubectl apply -f - <<EOF apiVersion: v1 kind: Service metadata: name: dev-vault spec: ports: - name: http protocol: TCP port: 8200 targetPort: 8200 --- apiVersion: v1 kind: Endpoints metadata: name: dev-vault subsets: - addresses: - ip: $(docker inspect dev-vault | jq -r \".[0].NetworkSettings.Networks.\\\"${DOCKER_NETWORK}\\\".IPAddress\") ports: - name: http port: 8200 protocol: TCP EOF Note Depending on your version of the tools and overall setup, this might not be needed but we've included it to make the tutorial stable.","title":"Configuring service entry for Vault"},{"location":"tutorials/vault-k8s/sidecar-injector-install/#installing-the-helm-chart","text":"Install Vault Helm chart which connects to the external Vault. Note that dev-vault is the name we used for the service, and is also the name of the docker container. helm repo add hashicorp https://helm.releases.hashicorp.com helm repo update helm install vault hashicorp/vault \\ --set \"global.externalVaultAddr=http://dev-vault:8200\" \\ --set \"csi.enabled=true\" \\ --version 0 .22.1","title":"Installing the Helm chart"}]}